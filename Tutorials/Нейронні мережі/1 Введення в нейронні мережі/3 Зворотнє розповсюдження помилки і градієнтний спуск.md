# Зворотнє розповсюдження помилки і градієнтний спуск

Основний спосіб тренування нейромереж - це тренування з вчителем. В нас є якийсь набір даних (входи і бажані виходи), нам треба подати всі ці входи в модель і порівняти з бажаними результатами. За допомогою функції втрат (Loss) ми можемо оцінити помилку моделі.

Існує багато функцій втрат, проста функція втрат порівнює різницю передбаченого виходу і бажаного виходу, також дає градієнт (напрямок зміни кожного виходу для **збільшення** втрати).
Розглянемо популярні:
```
# Mean Squared Error (MSE)
Loss = sum((y - target)^2)/n
dLoss/dy = 2(y - target)/n
dLoss/dtarget = 2(target - y)/n

# Mean Absolute Error (MAE)
Loss = sum(abs(y - target))/n
dLoss/dy = 1 if y > target; -1 if y < target; else 0
dLoss/dtarget = -1 if y > target; 1 if y < target; else 0

# Cross Entropy Loss
# Binary Cross Entropy
```

Маючи градієнт ми можемо зменшити помилку.
Зворотнє розповсюдження помилки передає градієнт (помилку) від функції втрат до кожного параметру і входу
Градієнт визначається правилом ланцюжка (перемноження похідних) і показує dLoss/dW (як зміна параметру впливає на зміну помилки)

Правило ланцюжка:
```
dy/dx = dy/dx * dz/dy
```

Похідні для стандартной функції нейрона:
```
z = x * w + b
y = act(z)
L = Loss_f(y, target)

dL/dy = Loss_f'(y, target)
dL/dz = dL/dy * dy/dz = Loss_f'(y, target) * act'(z)

grad_w = dL/dw = dL/dz * dz/dw
grad_b = dL/db = dL/dz * dz/db
grad_x = dL/dx = dL/dz * dz/dx

dz/dw = x
dz/db = 1
dz/dx = w
```

При послідовному з'єднанні градієнт буде розповсюджуватися назад (grad_l1_y = grad_l2_x)
Після визначення всіх частин градієнту ми можемо оптимізувати параметри через градыэнтний спуск.

В коді нам необхідно реалізувати зворотній прохід:
```python
class Linear:
	def __init__(self, n_inp, n_out):
		self.w = np.random.uniform(-0.1, 0.1, (n_inp, n_out))
		self.b = np.random.uniform(-0.1, 0.1, (1, n_out))

	def forward(self, x):
		return np.matmul(x, self.w) + self.b

    def backward(self, grad):
        self.grad_w = self.x.T @ grad
        self.grad_b = np.sum(grad, axis=0, keepdims=True)
        return grad @ self.w.T

class Sequential:
    def __init__(self, layers):
        self.layers = layers

    def forward(self, x):
        for layer in self.layers:
            x = layer.forward(x)
        return x

    def backward(self, grad):
        for layer in reversed(self.layers):
            grad = layer.backward(grad)
        return grad

    def __call__(self, x):
    	return self.forward(x)
```

Маючи всі градієнти ми можемо зробити крок оптимізації. Ми зміщуємо параметри на невелику відстань (learning rate) в протилежному напрямку до градієнту.

```
w(t+1) = w(t) - lr * grad_w
b(t+1) = b(t) - lr * grad_b
```