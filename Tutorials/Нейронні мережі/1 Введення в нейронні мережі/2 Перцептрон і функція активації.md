# Перцептрон і функція активації

Перцептрон - це нейронна мережа, яка складається з послідовно з'єднаних лінійних шарів, між ними зазвичай додається функція активації. Функція активації потрібна для апроксимації нелінійних функції, і обмеження чи нормалізації вихідних значень. Для зручності функцію активації рахують окремим шаром. Для тренування мережі через градієнтний спуск функція активації повинна бути непреривною, також нам потрібно знати її похідну.

Популярні функції активації:
```
# y' == dy/dx (похідна)

# Sigmoid (може давати затухання градієнту)
y = 1 / (1 + e^x)
y' = y * (1 - y)
# y Є (0; 1)

# Tanh
y = (e^x - e^(-x)) / (e^x + e^(-x))
y' = 1 - y^2
# y Є (-1; 1)

# ReLU
y = max(0, x)
y' = 1 if x > 0; else 0

# LeakyReLU
a = 0.01 (fixed)
y = max(a*x, x)
y' = 1 if x > 0; else a

# PReLU
a = 0.01 (parameter)
y = max(a*x, x)
y' = 1 if x > 0; else a
dy/da = 0 if x > 0; else x (PReLU змінює параметр підчас навчання)

# SoftPlus
# SoftMax
# Swish
```

Реалізуємо одну з наведених фунцій:
```python
class LeakyReLU:
	def forward(self, x, alpha=0.01):
		self.x = x
		self.alpha = alpha
		return np.where(x > 0, x, x * alpha)

	def backward(self, grad):
		return np.where(self.x > 0, 1, self.alpha) * grad
```

Використовуючи лінійні шари і шар активації можемо побудувати перцептрон для 3 входів і 2 виходів, в середині буде 5 нейронів:
```python
x = [[-0.1, 1.2, 0.5]]

layer1 = Linear(3, 5)
act1 = LeakyReLU(0.1)

layer2 = Linear(5, 2)
act2 = LeakyReLU(0.1)

# forward
x = act1(layer1(x))
x = act2(layer2(x))
y = x
```

Якщо архітектура не вимагає проміжних результатів, ми можемо об'єднати всі шари в послідовну модель:
```python
class Sequential:
    def __init__(self, layers):
        self.layers = layers

    def forward(self, x):
        for layer in self.layers:
            x = layer.forward(x)
        return x

    def __call__(self, x):
    	return self.forward(x)
```

І використовувати її як функцію:
```python
model = Sequential([
	Linear(3, 2),
	LeakyReLU(0.1),
	Linear(5, 2)
	LeakyReLU(0.1)
	])

x = [[-0.1, 1.2, 0.5]]

y = model(x)
```

Далі переходимо до тренування.