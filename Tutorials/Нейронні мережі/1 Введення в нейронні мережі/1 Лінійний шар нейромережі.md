# Штучний нейрон, лінійний шар

Нейронні мережі в якості штучного нейрону використовують лінійну параметричну функцію де вхід множиться на свою вагу, яка є параметром і буде змінюватися під час тренування мережі.

Один нейрон можна представити так:
```python
y = x * w
```
x - Вхід
w - Вага входу
y - Вихід

Такий нейрон може виконувати функцію конвертації з лінійною залежністю, наприклад метри в кілометри

В нейрона може бути будь-яка фіксована кількість входів, тоді для кожного буде своя вага:
```python
y = x1 * w1 + x2 * w2 + x3 * w3
```

Це можна представити в вигляді масивів однакової довжини і скористатися скалярним добутком:
```python
x = [x1, x2, x3]
w = [w1, w2, w3]

y = dot(x, w)
```
*dot не є вбудованою функцією*

Тепер ми можемо створити ще один нейрон для другого виходу:
```python
x = [x1, x2, x3]

w1 = [w11, w12, w13]
w2 = [w21, w22, w23]

y1 = dot(x, w1)
y2 = dot(x, w2)
```

Або об'єднати ваги в матрицю і використати матричне множення, матриця ваг повинна бути розміром (n_inp, n_out):
```python
x = [[x1, x2, x3]] #(1, 3)

w = [[w11, w12],
	 [w21, w22],
	 [w31, w32]] #(3, 2)

y = matmul(x, w) #(1, 2)
```
*matmul не є вбудованою функцією*

Ми отримали лінійний шар нейронної мережі, але при значеннях 0 на всіх входах вихід буде також рівний 0. Щоб виправити цей недостаток ми можемо додавати зміщення (bias) після множення входів на вагу. Це зміщення також є параметром і змінюється під час тренування. Розмір масиву зміщень має бути рівним кількості виходів шару.

```python
b = [[b1, b2]] #(1, 2)
y = matmul(x, w) + b #(1, 2)
```

Для використання в майбутньому створимо клас:
```python
import numpy as np

class Linear:
	def __init__(self, n_inp, n_out):
		self.w = np.random.uniform(-0.1, 0.1, (n_inp, n_out))
		self.b = np.random.uniform(-0.1, 0.1, (1, n_out))

	def forward(self, x):
		return np.matmul(x, self.w) + self.b
```
На старті задаємо випадкові параметри.